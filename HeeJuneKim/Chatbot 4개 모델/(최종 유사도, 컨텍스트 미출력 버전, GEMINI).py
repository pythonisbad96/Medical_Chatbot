# -*- coding: utf-8 -*-
"""
medical_main.py (V5.0: Google Gemini(1.5 Flash)ë¡œ LLM êµì²´ + ì¶œë ¥ ê°„ì†Œí™” ìœ ì§€)
- í„°ë¯¸ë„ì— ì¶œë ¥ë˜ë˜ 1ì°¨ ê²€ìƒ‰ ê²°ê³¼(ìœ ì‚¬ë„)ì™€ LLM ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ì œê±°.
- ì‚¬ìš©ìì—ê²ŒëŠ” AIì˜ ì§ˆë¬¸ê³¼ ìµœì¢… ë‹µë³€ë§Œ ë³´ì´ë„ë¡ ìœ ì§€.
- LLM í´ë¼ì´ì–¸íŠ¸: Google Generative AI (Gemini) ì‚¬ìš©.
"""

# ------------------------------------------------------------
# 0) í‘œì¤€/ì„œë“œíŒŒí‹° ëª¨ë“ˆ ì„í¬íŠ¸
# ------------------------------------------------------------
import os, json, re                    # os: ê²½ë¡œ/í™˜ê²½ë³€ìˆ˜, json: íŒŒì¼ ì½ê¸°, re: ì •ê·œí‘œí˜„ì‹
from typing import List, Tuple          # íƒ€ì… íŒíŠ¸ìš©

# LangChain, Google ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_huggingface import HuggingFaceEmbeddings  # í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”© ëª¨ë“ˆ
from langchain_community.vectorstores import FAISS      # FAISS ë²¡í„°DB
import google.generativeai as genai     # Google Gemini í´ë¼ì´ì–¸íŠ¸

# .env íŒŒì¼ ë¡œë“œ (ì„ íƒ)
try:
    from dotenv import load_dotenv
    load_dotenv()                       # .envë¥¼ ì½ì–´ í™˜ê²½ë³€ìˆ˜ë¡œ ë“±ë¡(ì—†ì–´ë„ í†µê³¼)
except Exception:
    pass

# GPU ì„¤ì •: torchê°€ ìˆìœ¼ë©´ CUDA ì‚¬ìš©, ì—†ìœ¼ë©´ CPU ì‚¬ìš©
try:
    import torch
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
except Exception:
    DEVICE = "cpu"


# ------------------------------------------------------------
# 1) ê²½ë¡œ/DB ê¸°ë³¸ ì„¤ì •
# ------------------------------------------------------------
JSON_FOLDER   = os.getenv("JSON_FOLDER", "./json_diseases_final_ver").strip()
_default_dbdir = f"vector_unified_{os.path.basename(JSON_FOLDER) or 'db'}"
DB_DIR        = os.getenv("DB_DIR", _default_dbdir).strip()

UNIFIED_DB_PATH  = f"{DB_DIR}/faiss_unified_disease_db"   # í†µí•© ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ
os.makedirs(DB_DIR, exist_ok=True)                       # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±


# ------------------------------------------------------------
# 2) ì‹¤í–‰ ì˜µì…˜ ë° ê¸°ì¤€ê°’(Threshold) ì„¤ì •
# ------------------------------------------------------------
FORCE_REBUILD   = os.getenv("FORCE_REBUILD", "0") == "1"   # 1ì´ë©´ í•­ìƒ ì¸ë±ìŠ¤ ì¬ìƒì„±
K_DISEASE    = int(os.getenv("K_DISEASE", "10"))           # ê²€ìƒ‰ ìƒìœ„ kê°œ ë¬¸ì„œ
MAX_DISEASES = int(os.getenv("MAX_DISEASES", "5"))        # LLMì— íˆ¬ì…í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
CTX_CHARS    = int(os.getenv("CTX_CHARS", "8000"))         # LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ(ë¬¸ììˆ˜)

LOW_CONF_THRESHOLD = 0.5    # ë¹„ì˜ë£Œ/ì¼ë°˜ ì‘ë‹µ ë¼ìš°íŒ… ê¸°ì¤€
HIGH_CONF_THRESHOLD = 0.74  # í™•ì‹ ë„ ë†’ìŒ ê¸°ì¤€
SCORE_DIFF_THRESHOLD = 0.03 # ìƒìœ„ ê²°ê³¼ ê°„ ì ìˆ˜ ì°¨ì´ ê¸°ì¤€


# ------------------------------------------------------------
# 3) ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
# ------------------------------------------------------------
# kosimcseë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ëŠ” ì´ì „ ìš”ì²­ì„ ê¸°ì–µí•˜ì—¬ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.
# ë§Œì•½ kosimcse-roberta-base-v1 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ê¸°ì¡´ ë¼ì¸ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
# EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "BM-K/kosimcse-roberta-base-v1").strip()
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "jhgan/ko-sroberta-multitask").strip()
embedding_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL_NAME,
    model_kwargs={"device": DEVICE},
    encode_kwargs={"normalize_embeddings": True, "batch_size": 64},
)


# ------------------------------------------------------------
# 4) Google Gemini LLM í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„
# ------------------------------------------------------------
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    # ì‹¤í–‰ ì „ .envì— GEMINI_API_KEY=... ì¶”ê°€ ë˜ëŠ” OS í™˜ê²½ë³€ìˆ˜ ë“±ë¡ í•„ìš”
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEY ì´(ê°€) ë¹„ì—ˆìŠµë‹ˆë‹¤. .env ë˜ëŠ” OS í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•˜ì„¸ìš”.")

genai.configure(api_key=GEMINI_API_KEY)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash").strip()

# Gemini API í˜¸ì¶œì„ ìœ„í•œ ì„¤ì •
generation_config = {
    "temperature": 0.3,
    "max_output_tokens": 2048,
}
safety_settings = [ # ëª¨ë“  ì•ˆì „ ì„¤ì •ì„ ë³´í†µ ì´ìƒìœ¼ë¡œ ì¡°ì •
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]

llm_model = genai.GenerativeModel(
    model_name=GEMINI_MODEL,
    generation_config=generation_config,
    safety_settings=safety_settings
)

def chat_with_gemini(messages, **gen_opts) -> str:
    """
    Google Gemini Chat Completions í˜¸ì¶œ
    - messages: [{"role": "...", "content": "..."}]
    - gen_opts: temperature, max_tokens ë“± (í˜„ì¬ëŠ” ì „ì—­ ì„¤ì • ì‚¬ìš©)
    """
    # GeminiëŠ” 'system' ì—­í• ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, user/modelë¡œ ë²ˆê°ˆì•„ ë‚˜íƒ€ë‚˜ë„ë¡ ë³€í™˜
    # ë˜í•œ, ì—°ì†ì ì¸ 'user' ë©”ì‹œì§€ëŠ” í•˜ë‚˜ë¡œ í•©ì³ì•¼ í•¨
    gemini_history = []
    current_user_content = []

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì²« ë²ˆì§¸ user ë©”ì‹œì§€ì— í•©ì¹¨
    system_prompt = next((m['content'] for m in messages if m['role'] == 'system'), "")
    if system_prompt:
        current_user_content.append(system_prompt)

    for msg in messages:
        if msg['role'] == 'user':
            current_user_content.append(msg['content'])
        elif msg['role'] == 'assistant' or msg['role'] == 'model':
            # ì´ì „ê¹Œì§€ì˜ user ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ì˜ íŒŒíŠ¸ë¡œ í•©ì³ì„œ ì¶”ê°€
            if current_user_content:
                gemini_history.append({'role': 'user', 'parts': ["\n".join(current_user_content)]})
                current_user_content = []
            # model(assistant)ì˜ ì‘ë‹µ ì¶”ê°€
            gemini_history.append({'role': 'model', 'parts': [msg['content']]})

    # ë§ˆì§€ë§‰ ë‚¨ì€ user ë©”ì‹œì§€ ì¶”ê°€
    if current_user_content:
        gemini_history.append({'role': 'user', 'parts': ["\n".join(current_user_content)]})
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
    prompt = gemini_history.pop() if gemini_history else {'role': 'user', 'parts': [""]}
    
    chat_session = llm_model.start_chat(history=gemini_history)
    response = chat_session.send_message(prompt['parts'])
    return response.text.strip()


# ------------------------------------------------------------
# 5) LLM ë³´ì¡° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ)
# ------------------------------------------------------------
def extract_any(section_val) -> str:
    """
    JSONì˜ ê° í•„ë“œ(ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸/ë”•ì…”ì…”ë¦¬)ë¥¼ ì¼ê´€ë˜ê²Œ ë¬¸ìì—´ë¡œ í’€ì–´ë‚´ëŠ” í•¨ìˆ˜.
    """
    if section_val is None:
        return ""
    if isinstance(section_val, dict):
        texts = []
        if 'supplement' in section_val:
            supp = section_val.get('supplement')
            if supp and isinstance(supp, list):
                texts.append("\n".join(str(x) for x in supp if x and str(x) != "None"))
        for k, v in section_val.items():
            if k == 'supplement':
                continue
            if isinstance(v, list):
                texts.append("\n".join(str(x) for x in v if x and str(x) != "None"))
            elif v and str(v) != "None":
                texts.append(str(v))
        return "\n".join(texts).strip()
    if isinstance(section_val, list):
        return "\n".join(str(x) for x in section_val if x and str(x) != "None").strip()
    return str(section_val).strip()

def get_disease_from_doc(doc):
    """ FAISS ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œì—ì„œ ë³‘ëª… ë©”íƒ€ë°ì´í„°ë¥¼ ì•ˆì „íˆ êº¼ë‚´ê¸° """
    return getattr(doc, "metadata", {}).get("ë³‘ëª…", "ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë³‘")

def extract_numbered_block(answer: str) -> str:
    """
    LLMì˜ ì „ì²´ ë‹µë³€ ì¤‘ '1. ... 2. ...' í˜•íƒœë¡œ ì‹œì‘í•˜ëŠ” ë²ˆí˜¸ ëª©ë¡ë§Œ ê¹”ë”íˆ ì¶”ì¶œ.
    í•­ëª©ì´ ë„ˆë¬´ ë§ìœ¼ë©´ 3~6ê°œë¡œ ì œí•œí•˜ì—¬ ê°„ê²°í™”.
    """
    items = re.findall(r'(?ms)^\s*\d\.\s.*?(?=^\s*\d\.|\Z)', answer)
    items = [it.strip() for it in items if it.strip()]
    if not items:
        return answer.strip()
    if len(items) >= 6:
        keep = items[:6]
    elif len(items) >= 5:
        keep = items[:5]
    elif len(items) >= 3:
        keep = items[:3]
    else:
        keep = items
    return "\n".join(keep).strip()


# ------------------------------------------------------------
# 6) FAISS í—¬í¼ ë° ë‹¨ì¼ ì¸ë±ìŠ¤ êµ¬ì¶• í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
# ------------------------------------------------------------
def faiss_from_texts(texts, embedding_model, metadatas=None):
    """ LangChain ë²„ì „ì— ë”°ë¼ ì¸ìëª…ì´ ë‹¤ë¥¸ ì´ìŠˆë¥¼ try/exceptë¡œ ì–‘ìª½ ì§€ì› """
    try:
        return FAISS.from_texts(texts, embedding=embedding_model, metadatas=metadatas)
    except TypeError:
        return FAISS.from_texts(texts, embeddings=embedding_model, metadatas=metadatas)

def faiss_load_local(path, embedding_model):
    """ ì €ì¥ëœ ë¡œì»¬ ì¸ë±ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ(ì—­ì§ë ¬í™” í—ˆìš©) """
    try:
        return FAISS.load_local(path, embedding=embedding_model, allow_dangerous_deserialization=True)
    except TypeError:
        return FAISS.load_local(path, embeddings=embedding_model, allow_dangerous_deserialization=True)

def _index_file(path: str) -> str:
    return os.path.join(path, "index.faiss")

def _latest_json_mtime(folder: str) -> float:
    """ í´ë” ë‚´ JSON íŒŒì¼ë“¤ì˜ ë§ˆì§€ë§‰ ìˆ˜ì •ì‹œê° ì¤‘ ìµœëŒ“ê°’ """
    times = []
    if not os.path.isdir(folder):
        return 0.0
    for f in os.listdir(folder):
        if f.lower().endswith(".json"):
            times.append(os.path.getmtime(os.path.join(folder, f)))
    return max(times) if times else 0.0

def _needs_rebuild(index_path: str, source_folder: str) -> bool:
    """ ì¸ë±ìŠ¤ê°€ ì—†ê±°ë‚˜, ë°ì´í„°ê°€ ë” ìµœì‹ ì´ë©´ ì¬ë¹Œë“œ í•„ìš” """
    if FORCE_REBUILD:
        return True
    idx = _index_file(index_path)
    if not os.path.exists(idx):
        return True
    return os.path.getmtime(idx) < _latest_json_mtime(source_folder)

def build_or_load_unified_disease_db():
    """
    í†µí•© ì§ˆë³‘ ì¸ë±ìŠ¤ ìƒì„±/ë¡œë”©
    """
    if _needs_rebuild(UNIFIED_DB_PATH, JSON_FOLDER):
        print("[Rebuild] í†µí•© ì§ˆë³‘ ì¸ë±ìŠ¤ (ê²€ìƒ‰ìš©/LLMìš© ë¶„ë¦¬)ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        texts_for_embedding, metas = [], []
        files = sorted([f for f in os.listdir(JSON_FOLDER) if f.endswith(".json")])

        for filename in files:
            with open(os.path.join(JSON_FOLDER, filename), encoding="utf-8") as f:
                data = json.load(f)

            disease_name = (data.get("ë³‘ëª…") or "").strip()
            if not disease_name:
                continue

            symptom_data = data.get("ì¦ìƒ", {})
            symptom_text = extract_any(symptom_data)

            supplement_text = ""
            if isinstance(symptom_data, dict) and 'supplement' in symptom_data:
                supplement_text = extract_any(symptom_data.get('supplement'))

            weighted_symptom_part = (f"[ì¦ìƒ] {symptom_text}\n" + f"[ì¦ìƒ.supplement] {supplement_text}\n") * 3

            other_info_parts = [f"[ë³‘ëª…] {disease_name}"]
            for key, value in data.items():
                if key not in ["ë³‘ëª…", "ì¦ìƒ"]:
                    content = extract_any(value)
                    if content:
                        other_info_parts.append(f"[{key}] {content}")
            other_info_part = "\n".join(other_info_parts)

            weighted_document_text = (weighted_symptom_part + other_info_part).strip()
            texts_for_embedding.append(weighted_document_text)

            clean_symptom_part = f"[ì¦ìƒ] {symptom_text}"
            clean_document_text = (clean_symptom_part + "\n" + other_info_part).strip()
            metas.append({"ë³‘ëª…": disease_name, "íŒŒì¼": filename, "clean_text": clean_document_text})

        if not texts_for_embedding:
            raise RuntimeError("í†µí•© ì¸ë±ìŠ¤ë¥¼ ë§Œë“¤ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        db = faiss_from_texts(texts_for_embedding, embedding_model, metadatas=metas)
        db.save_local(UNIFIED_DB_PATH)
        return db
    else:
        print("[Load] ê¸°ì¡´ í†µí•© ì§ˆë³‘ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        return faiss_load_local(UNIFIED_DB_PATH, embedding_model)


# ------------------------------------------------------------
# 7) ë‹¨ì¼ ê²€ìƒ‰ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
# ------------------------------------------------------------
def search_unified_db_with_scores(db, user_query: str, k: int) -> List[Tuple[any, float]]:
    """
    ì§ˆì˜ì–´ë¥¼ ë°›ì•„ ìƒìœ„ kê°œ ë¬¸ì„œì™€ ê±°ë¦¬ ì ìˆ˜ë¥¼ í•¨ê»˜ ë°˜í™˜.
    """
    q_fmt = f"query: {user_query}"
    if not db:
        return []
    return db.similarity_search_with_score(q_fmt, k)


# ------------------------------------------------------------
# 8) í”„ë¡¬í”„íŠ¸(SYSTEM) ë° ë©”ì¸ ë£¨í”„
# ------------------------------------------------------------
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸ì´ ê±´ê°•/ì¦ìƒ/ì˜í•™ ê´€ë ¨ì´ë©´, ì•„ë˜ [ì§ˆë³‘ ì •ë³´]ë¥¼ ì°¸ê³ í•˜ì—¬ 'ì¶œë ¥ í˜•ì‹'ì— ë§ì¶° ë‹µë³€í•˜ì„¸ìš”.
'ìƒë¹„ì•½ ì¶”ì²œ'ì€ ë‹¹ì‹ ì˜ ì˜ë£Œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
ë¶ˆí•„ìš”í•œ ì„œë¡ /ê²°ë¡  ì—†ì´ 'ì¶œë ¥ í˜•ì‹'ì˜ í•­ëª©ë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
1. ì˜ˆìƒë˜ëŠ” ë³‘ëª… (2~3ê°€ì§€):
   - ì²« ë²ˆì§¸ ë³‘ëª…ì€ **êµµê²Œ** í‘œê¸°í•˜ê³  ê°„ë‹¨í•œ ì„¤ëª…ë„ í¬í•¨í•˜ì„¸ìš”.
2. ì£¼ìš” ì›ì¸:
3. ì¶”ì²œ ì§„ë£Œê³¼ (2~3ê³¼):
4. ì˜ˆë°© ë° ê´€ë¦¬ ë°©ë²•:
5. ìƒí™œ ì‹œ ì£¼ì˜ì‚¬í•­:
6. ìƒë¹„ì•½ ì¶”ì²œ(ì‹¤ì œ ì œí’ˆ):
""".strip()

if __name__ == "__main__":
    # 1) ì¸ë±ìŠ¤ ì¤€ë¹„
    disease_db = build_or_load_unified_disease_db()
    print("\nâœ… í†µí•© ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")

    # 2) ëŒ€í™” ë£¨í”„
    while True:
        user_input = input("\nğŸ©º ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ").strip()
        if user_input.lower() in ["exit", "ì¢…ë£Œ", "quit"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 2-1) ê²€ìƒ‰ ìˆ˜í–‰
        docs_with_scores = search_unified_db_with_scores(disease_db, user_input, k=K_DISEASE)

        # 2-2) ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ: ì¼ë°˜ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
        if not docs_with_scores:
            print("[Info] ê´€ë ¨ ì§ˆë³‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            general_messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": user_input},
            ]
            try:
                answer = chat_with_gemini(general_messages)
                print(f"\nğŸ§¾ [Gemini ë‹µë³€]\n{answer}")
            except Exception as e:
                print(f"[ì˜¤ë¥˜] API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            continue

        # 2-3) ê±°ë¦¬ â†’ ê°„ì´ ìœ ì‚¬ë„(1/(1+dist))ë¡œ ë³€í™˜
        docs_with_sim_scores = [(doc, 1 / (1 + score)) for doc, score in docs_with_scores]
        unique_docs = docs_with_sim_scores

        # 2-4) ìƒìœ„ 1ê°œ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë¼ìš°íŒ… íŒë‹¨
        top1_doc, top1_score = unique_docs[0]
        final_docs = []

        # (A) ë¹„ì˜ë£Œ/ì¡ë‹´ ë¼ìš°íŒ…
        if top1_score < LOW_CONF_THRESHOLD:
            print(f"[íŒë‹¨] ë¹„ì˜ë£Œ ì§ˆë¬¸ (ìœ ì‚¬ë„: {top1_score:.2f} < {LOW_CONF_THRESHOLD})")
            general_messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": user_input},
            ]
            try:
                answer = chat_with_gemini(general_messages)
                print(f"\nğŸ§¾ [Gemini ë‹µë³€]\n{answer}")
            except Exception as e:
                print(f"[ì˜¤ë¥˜] API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            continue

        # (B) í™•ì‹ ë„ íŒë‹¨
        is_confident = (
            top1_score >= HIGH_CONF_THRESHOLD
            and (len(unique_docs) < 3 or (unique_docs[0][1] - unique_docs[2][1]) >= SCORE_DIFF_THRESHOLD)
        )

        if is_confident:
            print(f"[íŒë‹¨] í™•ì‹ ë„ ë†’ìŒ (ìœ ì‚¬ë„: {top1_score:.2f})")
            final_docs = [doc for doc, score in unique_docs[:MAX_DISEASES]]
        else:
            # (C) í™•ì‹ ë„ ë‚®ìŒ â†’ ì¶”ê°€ ì¦ìƒ ìš”ì²­ í›„ ì¬ê²€ìƒ‰
            print(f"[íŒë‹¨] í™•ì‹ ë„ ë‚®ìŒ (ìœ ì‚¬ë„: {top1_score:.2f}). ì¶”ê°€ ì¦ìƒì„ ìš”ì²­í•©ë‹ˆë‹¤.")
            print(f"\nì¦ìƒì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”? ì¶”ê°€ì ì¸ ì¦ìƒì´ ìˆë‹¤ë©´ í•¨ê»˜ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            user_answer = input("[ì¶”ê°€ ì¦ìƒ ì…ë ¥]: ").strip()

            if user_answer:
                combined_input = f"{user_input}\nì¶”ê°€ ì •ë³´: {user_answer}"
                print("\n[Info] ì¶”ê°€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                final_search_res = search_unified_db_with_scores(disease_db, combined_input, k=K_DISEASE)
                final_docs = [doc for doc, score in final_search_res[:MAX_DISEASES]]
            else:
                print("[Info] ì¶”ê°€ ì…ë ¥ì´ ì—†ì–´ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                final_docs = [doc for doc, score in unique_docs[:MAX_DISEASES]]

        # 2-5) LLM ìƒì„±: ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸(í´ë¦° ë²„ì „) + ì‚¬ìš©ì ì§ˆë¬¸
        if final_docs:
            final_context = "\n---\n".join([doc.metadata.get('clean_text', doc.page_content) for doc in final_docs])
            final_user_input = user_input
            if 'combined_input' in locals() and 'user_answer' in locals() and user_answer:
                final_user_input = combined_input

            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"[ì§ˆë³‘ ì •ë³´]\n{final_context[:CTX_CHARS]}"},
                {"role": "user", "content": final_user_input},
            ]

            try:
                answer = chat_with_gemini(messages)
                print("\nğŸ§¾ [Gemini ìµœì¢… ë‹µë³€]")
                print(extract_numbered_block(answer))
            except Exception as e:
                print(f"[ì˜¤ë¥˜] ìµœì¢… ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")