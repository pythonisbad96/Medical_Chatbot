from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
base = AutoModelForCausalLM.from_pretrained(
    "LGAI-EXAONE/EXAONE-4.0-1.2B",
    device_map="auto",
    load_in_4bit=True,
    trust_remote_code=True
)
model = PeftModel.from_pretrained(base, "./lora_exaone_adapter")
tokenizer = AutoTokenizer.from_pretrained("./lora_exaone_adapter", trust_remote_code=True)

# pipeline ìƒì„±
llm = pipeline("text-generation", model=model, tokenizer=tokenizer)

# í…ŒìŠ¤íŠ¸í•  ì¦ìƒ ë¦¬ìŠ¤íŠ¸
symptoms = [
    "ê¸°ì¹¨ì´ ê³„ì† ë‚˜ê³  ê°€ë˜ê°€ ë§ì•„ìš”",
    "ì—´ì´ ë‚˜ê³  ê·¼ìœ¡í†µì´ ì‹¬í•´ìš”",
    "ë‘í†µì´ ìˆê³  ë¹›ì— ì˜ˆë¯¼í•´ìš”",
    "ì†ì´ ë©”ìŠ¤ê»ê³  êµ¬í† ê°€ ìˆì–´ìš”",
    "ëª©ì´ ë”°ê°‘ê³  ìŒì‹ì„ ì‚¼í‚¤ê¸° í˜ë“¤ì–´ìš”",
    "ë°¤ë§ˆë‹¤ ìˆ¨ì´ ì°¨ê³  ê¸°ì¹¨ì´ ì‹¬í•´ìš”",
    "ì†ë°œì´ ì €ë¦¬ê³  í˜ˆë‹¹ì´ ë¶ˆì•ˆì •í•´ìš”",
    "ì†Œë³€ì„ ìì£¼ ë³´ê³  í†µì¦ì´ ìˆì–´ìš”",
    "ëˆˆì´ ê°€ë µê³  ì¬ì±„ê¸°ê°€ ìì£¼ ë‚˜ìš”",
    "ë³µí†µê³¼ ì„¤ì‚¬ê°€ ë©°ì¹ ì§¸ ì§€ì†ë¼ìš”"
]

# ëª¨ë¸ ì˜ˆì¸¡
for i, symptom in enumerate(symptoms, 1):
    prompt = f"ì¦ìƒ: {symptom}\nì§ˆë³‘:"
    result = llm(prompt, max_new_tokens=5, do_sample=False)
    output = result[0]['generated_text'].replace(prompt, "").strip()
    print(f"{i}. ì¦ìƒ: {symptom}")
    print(f"   ğŸ” ì˜ˆì¸¡ëœ ë³‘ëª…: {output}\n")
