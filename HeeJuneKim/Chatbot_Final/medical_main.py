# -*- coding: utf-8 -*-
"""
medical_main.py (ìµœì¢…ë³¸)

[ë¬´ì—‡ì„ í•˜ëŠ” íŒŒì¼ì¸ê°€ìš”?]
- ì½˜ì†”ì—ì„œ ì¦ìƒì„ ì…ë ¥ë°›ì•„ ì•„ë˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
  1) ì˜ë£Œì„± íŒë³„(ê°„ë‹¨ RAG í‰ê·  ìœ ì‚¬ë„)
  2) ì§ˆë³‘/ì•½í’ˆ ê²€ìƒ‰(ì¦ìƒ ì „ìš© ì¸ë±ìŠ¤ + E5 ì ‘ë‘ì‚¬ + similarity ë˜ëŠ” MMR)
  3) Evidence Gating(ê·¼ê±°ê°€ ì•½í•˜ë©´ ì•ˆì „ ëª¨ë“œ ì „í™˜)
  4) A.X 4.0(OpenAI í˜¸í™˜) í˜¸ì¶œë¡œ ìµœì¢… ë‹µë³€ ìƒì„±

[í•µì‹¬ ë¦¬íŠ¸ë¦¬ë²Œ ê°•í™” í¬ì¸íŠ¸]
- E5 ì ‘ë‘ì‚¬(query:/passage:) ì ìš©
- ì¦ìƒ ì „ìš© ì¸ë±ìŠ¤(ì¦ìƒ/ê´€ë ¨_ì¦ìƒ_í‘œí˜„ë§Œ ë¬¸ì¥ ë‹¨ìœ„) ë³„ë„ êµ¬ì¶•
- similarity ìš°ì„  + MMR í† ê¸€ ê°€ëŠ¥(í™˜ê²½ë³€ìˆ˜)
- êµ¬ì–´ì²´ â†’ ì˜í•™ì–´ ê°„ë‹¨ ì •ê·œí™”(CANON + JSON ì‚¬ì „)
- Evidence Gating(í‰ê· /ìµœëŒ€ ìœ ì‚¬ë„, ë¬¸ì„œ ìˆ˜ ì„ê³„)

[í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜ì„±]
- ì™¸ë¶€ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ê°€ ì´ ëª¨ë“ˆì„ import í•˜ì—¬ ì•„ë˜ í•¨ìˆ˜/ìƒìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
  (build_or_load_*, search_*, is_medical_by_rag, evidence_score/evidence_ok,
   get_disease_from_doc, dedupe_by_disease, embedding_model, SYSTEM_PROMPT/SAFE_PROMPT ë“±)

[í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜]
- ADOTX_API_KEY : A.X 4.0 API í‚¤ (ì—†ìœ¼ë©´ ì¦‰ì‹œ ì˜¤ë¥˜)

[í•„ìš” íŒ¨í‚¤ì§€]
- langchain_huggingface, langchain_community, openai, faiss, pandas, numpy, rapidfuzz, python-dotenv(ì˜µì…˜)
"""

# -------------------------
# í‘œì¤€/ì„œë“œíŒŒí‹° ëª¨ë“ˆ import
# -------------------------
import os                       # ê²½ë¡œ/í™˜ê²½ë³€ìˆ˜ ì²˜ë¦¬
import json                     # JSON ë¡œë“œ
import re                       # ì •ê·œí‘œí˜„ì‹(í…ìŠ¤íŠ¸ ê°€ê³µ)
import numpy as np              # ìˆ˜ì¹˜ì—°ì‚°(ìœ ì‚¬ë„ ê³„ì‚° ë“±)
import pandas as pd             # CSV ë¡œë“œ(ìƒë¹„ì•½)
from typing import List
from rapidfuzz import fuzz      # (í˜„ì¬ íŒŒì¼ì—ì„œëŠ” ì‚¬ìš© ë¹ˆë„ ë‚®ì§€ë§Œ ì¶”í›„ ìœ ìš©)

# ---- (ì„ íƒ) .env ì§€ì›: ë¡œì»¬ ê°œë°œ ì‹œ í¸í•˜ê²Œ í™˜ê²½ë³€ìˆ˜ ì£¼ì… ----
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ---- GPU ìë™ ê°ì§€: torchê°€ ìˆìœ¼ë©´ CUDA ì‚¬ìš©, ì—†ìœ¼ë©´ CPU ----
try:
    import torch
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
except Exception:
    DEVICE = "cpu"

# ---- LangChain: ì„ë² ë”© + FAISS ë²¡í„°DB ----
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# ---- A.X 4.0(OpenAI í˜¸í™˜ API) ----
from openai import OpenAI

# -------------------------
# ê²½ë¡œ/DB ë””ë ‰í† ë¦¬ ë“± ê¸°ë³¸ ì„¤ì •
# -------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))   # medical_main.py ìœ„ì¹˜ (src/)

# ë°ì´í„° í´ë” (ë£¨íŠ¸ ê¸°ì¤€ ../data)
JSON_FOLDER   = os.path.join(BASE_DIR, "../data/json_diseases_augmented")  # ì§ˆë³‘ JSON í´ë”
MED_CSV_PATH  = os.path.join(BASE_DIR, "../data/mfds_crawled_utf8.csv")   # ìƒë¹„ì•½ CSV
COLLOQUIAL_JSON = os.path.join(BASE_DIR, "../data/colloquial_to_disease.json")

# ë²¡í„° DB í´ë” (ë£¨íŠ¸ ê¸°ì¤€ ../vector_e5_large_with_meds)
DB_DIR        = os.path.join(BASE_DIR, "../vector_e5_large_with_meds")
DISEASE_DB_PATH = os.path.join(DB_DIR, "faiss_disease_db")
SYM_DB_PATH     = os.path.join(DB_DIR, "faiss_disease_db_symptom_only")
MED_DB_PATH     = os.path.join(DB_DIR, "faiss_med_db")
MEDICALNESS_DB  = os.path.join(DB_DIR, "faiss_medicalness")

os.makedirs(DB_DIR, exist_ok=True)              # ì—†ìœ¼ë©´ ìƒì„±

# -------------------------
# ëŸ°íƒ€ì„ í”Œë˜ê·¸(í™˜ê²½ë³€ìˆ˜ë¡œ í† ê¸€)
# -------------------------
USE_SYMPTOM_ONLY = os.getenv("USE_SYMPTOM_ONLY", "1") == "1"  # ì¦ìƒ ì „ìš© ì¸ë±ìŠ¤ ì‚¬ìš©(ê¸°ë³¸ ON)
USE_E5_PREFIX    = os.getenv("USE_E5_PREFIX", "1") == "1"     # E5 ì ‘ë‘ì‚¬ ì‚¬ìš©(ê¸°ë³¸ ON)
USE_MMR          = os.getenv("USE_MMR", "0") == "1"           # MMR í† ê¸€(ê¸°ë³¸ OFF)
LAMBDA_MULT      = float(os.getenv("LAMBDA_MULT", "0.2"))     # MMR ë‹¤ì–‘ì„± ê°€ì¤‘
FETCH_MULT       = int(os.getenv("FETCH_MULT", "5"))          # MMR fetch_k = k*FETCH_MULT

# -------------------------
# ì„ë² ë”© ëª¨ë¸(E5-large) ë¡œë“œ
# -------------------------
# normalize_embeddings=True â†’ ë‚´ì (dot) == ì½”ì‚¬ì¸ ìœ ì‚¬ë„
embedding_model = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": DEVICE},                      # âœ… GPU ìë™ê°ì§€ ë°˜ì˜
    encode_kwargs={"normalize_embeddings": True, "batch_size": 64},
)

# -------------------------
# ê²€ìƒ‰/ì»¨í…ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
# -------------------------
K_DISEASE    = 10    # ì§ˆë³‘ ê²€ìƒ‰ Top-k
K_MED        = 5     # ìƒë¹„ì•½ ê²€ìƒ‰ Top-k
MAX_DISEASES = 5     # ì»¨í…ìŠ¤íŠ¸ì— ë„£ì„ ì„œë¡œ ë‹¤ë¥¸ ë³‘ëª… ìˆ˜(ì¤‘ë³µ ì œê±° í›„)
MAX_MEDS     = 3     # ì»¨í…ìŠ¤íŠ¸ì— ë„£ì„ ìƒë¹„ì•½ ìˆ˜
CTX_CHARS    = 2000  # LLMì— ë³´ë‚¼ ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´
MED_TOPK     = 5     # ì˜ë£Œì„± íŒë³„ìš© ê²€ìƒ‰ k
MED_TH       = 0.55  # ì˜ë£Œì„± í‰ê·  ìœ ì‚¬ë„ ì„ê³„ê°’

# -------------------------
# A.X 4.0 í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (í‚¤ í•„ìˆ˜)
# -------------------------
AX_BASE_URL = "https://guest-api.sktax.chat/v1"
AX_API_KEY  = os.getenv("ADOTX_API_KEY", "").strip()  # âœ… ê¸°ë³¸ê°’ ì œê±°: ë°˜ë“œì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì£¼ì…
AX_MODEL    = "ax4"

if not AX_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ ADOTX_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. A.X 4.0 API í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")

client = OpenAI(base_url=AX_BASE_URL, api_key=AX_API_KEY)

def chat_with_ax(messages, **gen_opts):
    """
    A.X 4.0(chat.completions) í˜¸ì¶œ ìœ í‹¸ í•¨ìˆ˜
    - messages: [{"role":"system/user/assistant", "content":"..."}]
    - gen_opts: temperature, max_tokens ë“±(ì˜µì…˜)
    """
    completion = client.chat.completions.create(
        model=AX_MODEL,
        messages=messages,
        temperature=gen_opts.get("temperature", 0.3),
        max_tokens=gen_opts.get("max_tokens", 1024),
    )
    return completion.choices[0].message.content.strip()

# -------------------------
# êµ¬ì–´ì²´ â†’ ì˜í•™ì–´ ì •ê·œí™” ì‚¬ì „(JSON)
# -------------------------
try:
    with open(COLLOQUIAL_JSON, "r", encoding="utf-8") as f:
        COLLOQUIAL_MAP = json.load(f)
except FileNotFoundError:
    COLLOQUIAL_MAP = {}

def normalize_query_dict(q: str) -> str:
    """
    JSON ì‚¬ì „ ê¸°ë°˜ ì •ê·œí™”
    - ì‚¬ì „ ê°’ì´ list/tupleë¡œ ë˜ì–´ ìˆì–´ë„ ensure_textë¡œ ë¬¸ìì—´í™”í•´ì„œ ë°˜í™˜
    """
    key = ensure_text(q).strip()
    val = COLLOQUIAL_MAP.get(key, key)
    return ensure_text(val)

# -------------------------
# ê°„ë‹¨ ì¹˜í™˜(CANON): êµ¬ì–´ì²´ íŒ¨í„´ì„ ì˜í•™ ìš©ì–´ë¡œ êµì²´
# -------------------------
CANON = {
    "ìŒ•ìŒ•ê±°ë ¤ìš”":"ì²œëª…", "ìŒ•ìŒ•ê±°ë¦¼":"ì²œëª…", "ìŒ•ìŒ•":"ì²œëª…",
    "ê°€ìŠ´ì´ ë‹µë‹µ":"í‰ë¶€ ì••ë°•ê°", "ê°€ìŠ´ë‹µë‹µ":"í‰ë¶€ ì••ë°•ê°",
    "ìˆ¨ì´ ê°€ë¹ ":"í˜¸í¡ê³¤ë€", "ìˆ¨ì´ì°¨ìš”":"í˜¸í¡ê³¤ë€", "ìˆ¨ì´ì°¨":"í˜¸í¡ê³¤ë€",
    "í† í•  ê²ƒ ê°™":"ì˜¤ì‹¬", "ì†ì´ ìš¸ë ":"ì˜¤ì‹¬", "ë©”ìŠ¤êº¼":"ì˜¤ì‹¬",
    "í† í–ˆ":"êµ¬í† ", "êµ¬ì—­ì§ˆ":"êµ¬í† ",
    "ë‘ê·¼ë‘ê·¼":"ì‹¬ê³„í•­ì§„", "ì‹¬ì¥ì´ ë¹¨ë¦¬ ë›°":"ì‹¬ê³„í•­ì§„",
    "ì–´ì§€ëŸ¬ì›Œìš”":"í˜„í›ˆ", "ë¹™ê¸€ë¹™ê¸€":"í˜„í›ˆ", "ëˆˆì•ì´ í•‘":"í˜„í›ˆ",
    "ì½§ë¬¼ë‚˜":"ë¹„ë£¨", "ì½”ë§‰í˜€":"ë¹„íìƒ‰", "ì½”ë§‰í˜":"ë¹„íìƒ‰",
    "ëª©ì´ ì•„íŒŒ":"ì¸ë‘í†µ", "ëª©ë”°ê°€ì›€":"ì¸ë‘í†µ",
    "ê°€ë˜":"ê°ë‹´", "í”¼ê°€ë˜":"í˜ˆë‹´",
    "ì—´ì´ ë‚˜":"ë°œì—´", "ë¯¸ì—´":"ë°œì—´",
    "ì˜¤ì¤ŒëˆŒë•Œ ì•„íŒŒ":"ë°°ë‡¨í†µ", "ì†Œë³€ë³¼ë•Œ ì•„íŒŒ":"ë°°ë‡¨í†µ",
    "ìì£¼ ì†Œë³€":"ë¹ˆë‡¨", "ë°¤ì— ìì£¼ ì†Œë³€":"ì•¼ë‡¨",
    "ë°°ì•„íŒŒìš”":"ë³µí†µ", "ì†ì´ ì•„íŒŒ":"ë³µí†µ",
    "ì„¤ì‚¬í•´ìš”":"ì„¤ì‚¬", "ë¬½ì€ ë³€":"ì„¤ì‚¬", "í”¼ë³€":"í˜ˆë³€",
    "ì†ì“°ë ¤ìš”":"ê°€ìŠ´ì“°ë¦¼", "ì“°ë¦¼":"ê°€ìŠ´ì“°ë¦¼",
    "ê¸°ì¹¨ë‚˜ì™€ìš”":"ê¸°ì¹¨", "ê¸°ì¹¨ ì‹¬í•´ìš”":"ê¸°ì¹¨",
    "í”¼ë¶€ê°€ ê°€ë ¤":"ì†Œì–‘ì¦", "ê°€ë µ":"ì†Œì–‘ì¦",
    "ë¶€ì—ˆë‹¤":"ë¶€ì¢…", "ë¶“ê¸°":"ë¶€ì¢…",
}
def normalize_query_colloquial(q: str) -> str:
    """êµ¬ì–´ì²´ í‘œí˜„ì„ ì˜í•™ì–´ë¡œ ê°„ë‹¨ ì¹˜í™˜(ì…ë ¥ì€ ë¬´ì—‡ì´ ì™€ë„ ë¬¸ìì—´ë¡œ ì •ê·œí™”)"""
    s = ensure_text(q)
    for k, v in CANON.items():
        s = s.replace(k, v)
    return s


# -------------------------
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ìœ í‹¸
# -------------------------

# (íŒŒì¼ ìƒë‹¨ ìœ í‹¸ í•¨ìˆ˜ë“¤ ê·¼ì²˜ì— ì¶”ê°€)
def ensure_text(x) -> str:
    """
    ì–´ë–¤ íƒ€ì…ì´ ì™€ë„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - list/tuple/set: ê³µë°±ìœ¼ë¡œ í•©ì³ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ
    - ê·¸ ì™¸: str() ì‹œë„, ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´
    """
    if isinstance(x, str):
        return x
    if isinstance(x, (list, tuple, set)):
        return " ".join(map(str, x))
    try:
        return str(x)
    except Exception:
        return ""


def extract_text(field_dict):
    """
    JSON ì„¹ì…˜(dict)ì—ì„œ ë¬¸ìì—´ë§Œ ê¹”ë”íˆ ì¶”ì¶œ
    - ê°’ì´ listë©´ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì¹©ë‹ˆë‹¤.
    - "None"/ë¹ˆê°’ì€ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not isinstance(field_dict, dict):
        return ""
    texts = []
    for v in field_dict.values():
        if isinstance(v, list):
            vals = [str(x) for x in v if x not in (None, "", "None")]
            if vals:
                texts.append("\n".join(vals))
        elif isinstance(v, str):
            if v and v != "None":
                texts.append(v)
        elif v is not None:
            texts.append(str(v))
    return "\n".join(texts).strip()

def chunk_text(text: str, size: int = 500, overlap: int = 100) -> List[str]:
    """
    ë‹¨ìˆœ ìŠ¬ë¼ì´ë”© ì²­í‚¹: ê¸¸ì´ê°€ sizeë¥¼ ë„˜ìœ¼ë©´ overlapë§Œí¼ ê²¹ì¹˜ë©° ë¶„í• 
    """
    text = text or ""
    out, i, n = [], 0, len(text)
    while i < n:
        j = min(i + size, n)
        out.append(text[i:j])
        if j >= n: break
        i = max(0, j - overlap)
    return out

def split_sentences(text: str) -> List[str]:
    """
    ê°„ë‹¨ ë¬¸ì¥ ë¶„ë¦¬(ì˜ë¬¸ ë¬¸ì¥ë¶€í˜¸ ê¸°ì¤€, í•œêµ­ì–´ë„ ëŒ€ì²´ë¡œ ë™ì‘)
    """
    if not text:
        return []
    s = re.sub(r"\s+", " ", text).strip()
    return [x for x in re.split(r"(?<=[\.?!])\s+", s) if x]

# -------------------------
# ë¬¸ì„œì—ì„œ ë³‘ëª… ì¶”ì¶œ + ì¤‘ë³µ ì œê±°
# -------------------------
def get_disease_from_doc(doc):
    """
    ë²¡í„°DB ë¬¸ì„œ ë©”íƒ€ë°ì´í„°/ë³¸ë¬¸ì—ì„œ ë³‘ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - ë©”íƒ€ë°ì´í„° 'ë³‘ëª…' ìš°ì„ , ì—†ìœ¼ë©´ ë³¸ë¬¸ì—ì„œ [ë³‘ëª…] ë¼ì¸ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    if getattr(doc, "metadata", None) and "ë³‘ëª…" in doc.metadata:
        return doc.metadata["ë³‘ëª…"]
    m = re.search(r"\[ë³‘ëª…\]\s*(.*)", getattr(doc, "page_content", "") or "")
    return m.group(1).strip() if m else None

def dedupe_by_disease(docs, max_items: int) -> List:
    """
    ë™ì¼ ë³‘ëª… ì¤‘ë³µ ì œê±°: ê°™ì€ ë³‘ëª…ì€ í•œ ë²ˆë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    seen, uniq = set(), []
    for d in docs:
        dis = get_disease_from_doc(d) or "_unknown_"
        if dis in seen:
            continue
        seen.add(dis)
        uniq.append(d)
        if len(uniq) >= max_items:
            break
    return uniq

# -------------------------
# E5 ì ‘ë‘ì‚¬ í¬ë§·í„°
# -------------------------
def format_query_for_e5(q: str) -> str:
    """E5 ê¶Œì¥ í¬ë§·: ì¿¼ë¦¬ì— 'query:' ì ‘ë‘ì‚¬(í† ê¸€ ê°€ëŠ¥)"""
    return f"query: {q}" if USE_E5_PREFIX else q

# -------------------------
# ì¸ë±ìŠ¤: ì§ˆë³‘(ì „ì²´), ì§ˆë³‘(ì¦ìƒ ì „ìš©), ìƒë¹„ì•½, ì˜ë£Œì„±
# -------------------------
def faiss_from_texts(texts, embedding_model, metadatas=None):
    """
    LangChain FAISS ë²„ì „ ì°¨ì´ ëŒ€ì‘ ë˜í¼: embedding / embeddings ì¸ìëª…ì„ ëª¨ë‘ ì‹œë„
    """
    try:
        return FAISS.from_texts(texts, embedding=embedding_model, metadatas=metadatas)
    except TypeError:
        return FAISS.from_texts(texts, embeddings=embedding_model, metadatas=metadatas)

def faiss_load_local(path, embedding_model):
    """
    LangChain FAISS ë²„ì „ ì°¨ì´ ëŒ€ì‘ ë˜í¼: embedding / embeddings ì¸ìëª…ì„ ëª¨ë‘ ì‹œë„
    """
    try:
        return FAISS.load_local(path, embedding=embedding_model, allow_dangerous_deserialization=True)
    except TypeError:
        return FAISS.load_local(path, embeddings=embedding_model, allow_dangerous_deserialization=True)

def build_or_load_faiss_disease():
    """
    (ì „ì²´ ì„¹ì…˜) ì§ˆë³‘ ì¸ë±ìŠ¤: [ë³‘ëª…] + [ì •ì˜/ì›ì¸/ì¦ìƒ/ì§„ë‹¨/ì¹˜ë£Œ/ê´€ë ¨_ì¦ìƒ_í‘œí˜„]
    - ê° ì²­í¬ ì•ì— 'passage:' ì ‘ë‘ì‚¬(E5 ê¶Œì¥) ë¶€ì—¬
    """
    idx = os.path.join(DISEASE_DB_PATH, "index.faiss")
    if os.path.exists(idx):
        return faiss_load_local(DISEASE_DB_PATH, embedding_model)

    texts, metas = [], []
    files = sorted([f for f in os.listdir(JSON_FOLDER) if f.endswith(".json")])
    for filename in files:
        with open(os.path.join(JSON_FOLDER, filename), encoding="utf-8") as f:
            data = json.load(f)
        disease = (data.get("ë³‘ëª…") or "").strip()
        sections = [
            ("ì •ì˜",   extract_text(data.get("ì •ì˜", {}))),
            ("ì›ì¸",   extract_text(data.get("ì›ì¸", {}))),
            ("ì¦ìƒ",   extract_text(data.get("ì¦ìƒ", {}))),
            ("ì§„ë‹¨",   extract_text(data.get("ì§„ë‹¨", {}))),
            ("ì¹˜ë£Œ",   extract_text(data.get("ì¹˜ë£Œ", {}))),
            ("ê´€ë ¨_ì¦ìƒ_í‘œí˜„", "\n".join(data.get("ê´€ë ¨_ì¦ìƒ_í‘œí˜„", []))),
        ]
        for sec_name, sec_text in sections:
            if not sec_text:
                continue
            for j, ch in enumerate(chunk_text(sec_text)):
                text = f"passage: [ë³‘ëª…] {disease}\n[{sec_name}] {ch}".strip()
                texts.append(text)
                metas.append({"ë³‘ëª…": disease, "íŒŒì¼": filename, "ì„¹ì…˜": sec_name, "chunk_id": j})

    db = faiss_from_texts(texts, embedding_model, metadatas=metas)
    db.save_local(DISEASE_DB_PATH)
    return db

def build_or_load_faiss_disease_symptom_only():
    """
    (ì¦ìƒ ì „ìš©) ì§ˆë³‘ ì¸ë±ìŠ¤: [ì¦ìƒ] + [ê´€ë ¨_ì¦ìƒ_í‘œí˜„]ë§Œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥
    - ì •ì˜/ì§„ë‹¨/ì¹˜ë£Œ ë“± ë…¸ì´ì¦ˆë¥¼ ì œê±°í•´ ì¦ìƒ ì§ˆì˜ì— ë¯¼ê°
    - ê° ë¬¸ì¥ ì•ì— 'passage:' ì ‘ë‘ì‚¬ ë¶€ì—¬
    """
    idx = os.path.join(SYM_DB_PATH, "index.faiss")
    if os.path.exists(idx):
        return faiss_load_local(SYM_DB_PATH, embedding_model)

    texts, metas = [], []
    files = sorted([f for f in os.listdir(JSON_FOLDER) if f.endswith(".json")])
    for filename in files:
        with open(os.path.join(JSON_FOLDER, filename), encoding="utf-8") as f:
            data = json.load(f)
        disease = (data.get("ë³‘ëª…") or "").strip()

        raw_sym   = extract_text(data.get("ì¦ìƒ", {}))
        raw_exprs = "\n".join(data.get("ê´€ë ¨_ì¦ìƒ_í‘œí˜„", []))
        merged = (raw_sym + ("\n" if raw_sym and raw_exprs else "") + raw_exprs).strip()
        if not merged:
            continue

        # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬
        sents = re.split(r"(?<=[.!?])\s+|\n+", merged)
        for j, sent in enumerate([s for s in sents if s.strip()][:200]):
            text = f"passage: [ë³‘ëª…] {disease}\n[ì¦ìƒ] {sent.strip()}"
            texts.append(text)
            metas.append({"ë³‘ëª…": disease, "íŒŒì¼": filename, "ì„¹ì…˜": "ì¦ìƒ/í‘œí˜„", "chunk_id": j})

    db = faiss_from_texts(texts, embedding_model, metadatas=metas)
    db.save_local(SYM_DB_PATH)
    return db

def build_or_load_faiss_meds():
    """
    ìƒë¹„ì•½ ì¸ë±ìŠ¤: [ì œí’ˆëª…] + [íš¨ëŠ¥íš¨ê³¼]
    - ì¸ì½”ë”©ì„ utf-8-sig ìš°ì„  ì‹œë„í•˜ì—¬ ì—‘ì…€ CSV(BOM)ë„ ì•ˆì „í•˜ê²Œ ë¡œë“œ
    """
    idx = os.path.join(MED_DB_PATH, "index.faiss")
    if os.path.exists(idx):
        return faiss_load_local(MED_DB_PATH, embedding_model)

    # âœ… ì¸ì½”ë”© ì•ˆì „ ì²˜ë¦¬
    try:
        df = pd.read_csv(MED_CSV_PATH, encoding="utf-8-sig")
    except Exception:
        df = pd.read_csv(MED_CSV_PATH)

    texts, metas = [], []
    for _, row in df.iterrows():
        product_name = row.get("ì œí’ˆëª…", "")
        efficacy     = row.get("íš¨ëŠ¥íš¨ê³¼", "")
        if isinstance(product_name, str) and isinstance(efficacy, str) and efficacy.strip() and efficacy != "-":
            text = f"[ì œí’ˆëª…] {product_name}\n[íš¨ëŠ¥íš¨ê³¼] {efficacy}"
            texts.append(text)
            metas.append({"ì œí’ˆëª…": product_name, "ìƒì„¸ë§í¬": row.get("ìƒì„¸ë§í¬", "")})

    if not texts:
        raise ValueError("ìƒë¹„ì•½ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. CSVë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    db = faiss_from_texts(texts, embedding_model, metadatas=metas)
    db.save_local(MED_DB_PATH)
    return db

def build_or_load_medicalness_index():
    """
    ì˜ë£Œì„± íŒë³„ ì†Œí˜• ì¸ë±ìŠ¤(ì •ì˜/ì›ì¸/ì¦ìƒ/ì§„ë‹¨/ì¹˜ë£Œ/ê´€ë ¨_ì¦ìƒ_í‘œí˜„ì˜ ì¼ë¶€ ë¬¸ì¥)
    - ì§ˆë¬¸ì´ ì˜ë£Œ ê´€ë ¨ì¸ì§€ ì‚¬ì „ í•„í„°ë§í•˜ëŠ” ìš©ë„
    """
    os.makedirs(MEDICALNESS_DB, exist_ok=True)
    idx_file = os.path.join(MEDICALNESS_DB, "index.faiss")
    if os.path.exists(idx_file):
        return faiss_load_local(MEDICALNESS_DB, embedding_model)

    samples, metas = [], []
    files = [fn for fn in os.listdir(JSON_FOLDER) if fn.endswith(".json")]
    for filename in files:
        with open(os.path.join(JSON_FOLDER, filename), encoding="utf-8") as f:
            data = json.load(f)
        for sec in ("ì •ì˜", "ì›ì¸", "ì¦ìƒ", "ì§„ë‹¨", "ì¹˜ë£Œ", "ê´€ë ¨_ì¦ìƒ_í‘œí˜„"):
            # ì„¹ì…˜ì´ dictì´ë©´ extract_text, listì´ë©´ join
            txt = extract_text(data.get(sec, {})) if isinstance(data.get(sec), dict) else "\n".join(data.get(sec, []))
            for sent in split_sentences(txt)[:2]:  # ì„¹ì…˜ë‹¹ ìµœëŒ€ 2ë¬¸ì¥ ìƒ˜í”Œ
                if sent:
                    samples.append(sent)
                    metas.append({"íŒŒì¼": filename, "ì„¹ì…˜": sec})

    if not samples:
        raise RuntimeError("ì˜ë£Œì„± ì¸ë±ìŠ¤ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. JSON êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    db = faiss_from_texts(samples, embedding_model, metadatas=metas)
    db.save_local(MEDICALNESS_DB)
    return db

# -------------------------
# ì˜ë£Œì„± íŒë³„ (LangChain ë²„ì „ í˜¸í™˜)
# -------------------------
def is_medical_by_rag(text: str, retriever):
    """
    ì¿¼ë¦¬ì™€ ì˜ë£Œì„± ì¸ë±ìŠ¤ Top-k ë¬¸ì¥ ê°„ ì„ë² ë”© ì½”ì‚¬ì¸ í‰ê· ìœ¼ë¡œ ì˜ë£Œì„± íŒë³„
    - normalize_embeddings=True â†’ dot == cosine
    - retriever.invoke() ì—†ìœ¼ë©´ get_relevant_documents()ë¡œ í´ë°±(ë²„ì „ í˜¸í™˜)
    """
    if not text:
        return False

    # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    q_vec = np.array(embedding_model.embed_query(text), dtype=np.float32)

    # âœ… ë²„ì „ í˜¸í™˜: invoke â†’ ì‹¤íŒ¨ ì‹œ get_relevant_documents
    try:
        docs = retriever.invoke(text)
    except Exception:
        docs = retriever.get_relevant_documents(text)

    if not docs:
        return False

    # í›„ë³´ ë¬¸ì„œ ë²¡í„°ì™€ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    cand_vecs = embedding_model.embed_documents([d.page_content for d in docs])
    sims = [float(np.dot(q_vec, np.array(v, dtype=np.float32))) for v in cand_vecs]
    avg_sim = float(np.mean(sims))
    print(f"[MED-RAG] sims={', '.join(f'{s:.3f}' for s in sims)} | avg={avg_sim:.3f}")
    return avg_sim >= MED_TH

# -------------------------
# Evidence Gating (ê·¼ê±° ë¶€ì¡± ì‹œ ì•ˆì „ ëª¨ë“œ)
# -------------------------
EVIDENCE_MIN_AVG = float(os.getenv("EVIDENCE_MIN_AVG", "0.35"))  # í‰ê·  ìœ ì‚¬ë„ í•˜í•œ(ê¶Œì¥ ì‹œì‘ì )
EVIDENCE_MIN_MAX = float(os.getenv("EVIDENCE_MIN_MAX", "0.45"))  # ìµœëŒ€ ìœ ì‚¬ë„ í•˜í•œ
EVIDENCE_MIN_N   = int(os.getenv("EVIDENCE_MIN_N",   "3"))       # ìµœì†Œ ë¬¸ì„œ ìˆ˜

def evidence_score(docs, formatted_query: str):
    """
    ê²€ìƒ‰ ê²°ê³¼(docs)ì™€ ì¿¼ë¦¬(formatted_query: query: í¬í•¨) ê°„ì˜ ê·¼ì ‘ë„ë¥¼ ì¸¡ì •
    - ë°˜í™˜: (avg_sim, max_sim, n_docs)
    """
    if not docs:
        return 0.0, 0.0, 0
    q_vec = np.array(embedding_model.embed_query(formatted_query), dtype=np.float32)
    d_vecs = embedding_model.embed_documents([d.page_content for d in docs])
    sims = [float(np.dot(q_vec, np.array(v, dtype=np.float32))) for v in d_vecs]
    return float(np.mean(sims)), float(np.max(sims)), len(docs)

def evidence_ok(avg_sim, max_sim, n_docs) -> bool:
    """
    í‰ê· /ìµœëŒ€ ìœ ì‚¬ë„ì™€ ë¬¸ì„œ ìˆ˜ê°€ ì„ê³„ ì´ìƒì´ë©´ True â†’ ì¼ë°˜ ëª¨ë“œ
    ì•„ë‹ˆë©´ False â†’ ì•ˆì „ ëª¨ë“œ
    """
    if n_docs < EVIDENCE_MIN_N:    return False
    if avg_sim < EVIDENCE_MIN_AVG: return False
    if max_sim < EVIDENCE_MIN_MAX: return False
    return True

# -------------------------
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(ì¼ë°˜/ì•ˆì „)
# -------------------------
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜ë£Œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸ì´ ê±´ê°•/ì¦ìƒ/ì˜í•™ ê´€ë ¨ì´ë©´, ì•„ë˜ [ì§ˆë³‘ ì •ë³´]ì™€ [ìƒë¹„ì•½ ì •ë³´]ë¥¼ ì°¸ê³ í•˜ì—¬ 1~6ë²ˆ í•­ëª©ì„ ì‘ì„±í•˜ì„¸ìš”.
6ë²ˆì€ ë°˜ë“œì‹œ ì‹¤ì œ ìƒë¹„ì•½ ì œí’ˆëª…ì„ í™œìš©í•˜ì„¸ìš”. ì—†ìœ¼ë©´ 'ê´€ë ¨ ì¦ìƒì— ë§ëŠ” ìƒë¹„ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

# ì„œì‹ ê¸ˆì§€ ê·œì¹™(ì¤‘ìš”):
- ë§ˆí¬ë‹¤ìš´ì„ ì¼ì ˆ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. êµµê²Œ(**), ê¸°ìš¸ì„(*, _), í—¤ë”(#), ì½”ë“œë¸”ë¡(```), ì¸ë¼ì¸ì½”ë“œ(`) ëª¨ë‘ ê¸ˆì§€.
- íŠ¹íˆ ë³„í‘œ(*) ë¬¸ìëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ëª©ë¡ì€ '1. 2. 3.' ê°™ì€ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- í‰ë¬¸(plain text)ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ğŸ“ ì¶œë ¥ í˜•ì‹:
1. ì˜ˆìƒë˜ëŠ” ë³‘ëª… (2~3ê°€ì§€): (ì²« ë²ˆì§¸ ë³‘ëª…ì€ ê°„ë‹¨í•œ ì„¤ëª…ë„ í¬í•¨)
2. ì£¼ìš” ì›ì¸:
3. ì¶”ì²œ ì§„ë£Œê³¼ (2~3ê³¼):
4. ì˜ˆë°© ë° ê´€ë¦¬ ë°©ë²•:
5. ìƒí™œ ì‹œ ì£¼ì˜ì‚¬í•­:
6. ìƒë¹„ì•½ ì¶”ì²œ(ì‹¤ì œ ì œí’ˆ):
""".strip()

SAFE_PROMPT = """
ë‹¹ì‹ ì€ ì˜ë£Œ ì•ˆì „ ëª¨ë“œì˜ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.

ê·¼ê±°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œì—ëŠ”
- íŠ¹ì • ë³‘ëª… ì§„ë‹¨ ë˜ëŠ” ìƒë¹„ì•½ ì œí’ˆ ì¶”ì²œì„ í•˜ì§€ ë§ê³ ,
- ì¶”ê°€ ì •ë³´ ì§ˆë¬¸ê³¼, ì‘ê¸‰ ê²½ê³  ì‹ í˜¸/ë‚´ì› ê¸°ì¤€/ì¼ë°˜ì ì¸ ìƒí™œê´€ë¦¬ë§Œ ì•ˆë‚´í•˜ì„¸ìš”.
- ì¶œë ¥ì€ ìˆ«ì ëª©ë¡(1~)ì˜ í‰ë¬¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

ğŸ“ ì¶œë ¥ í˜•ì‹:
1. ì¶”ê°€ë¡œ í™•ì¸ì´ í•„ìš”í•œ ì •ë³´:
2. ì‘ê¸‰ì‹¤ ë˜ëŠ” ì¦‰ì‹œ ì§„ë£Œê°€ í•„ìš”í•œ ìœ„í—˜ ì‹ í˜¸:
3. ì¼ì‹œì ì¸ ìê°€ ê´€ë¦¬ ë°©ë²•(ì§„ë‹¨/ì•½ ì¶”ì²œ ê¸ˆì§€):
""".strip()

# -------------------------
# ê²€ìƒ‰ í•¨ìˆ˜ (similarity ê¸°ë³¸, í•„ìš” ì‹œ MMR)
# -------------------------
def search_diseases(db, user_query: str, k: int = K_DISEASE):
    """
    1) êµ¬ì–´ì²´â†’ì˜í•™ì–´ ê°„ë‹¨ ì¹˜í™˜
    2) E5 ê¶Œì¥ í¬ë§·(query:) ì ìš©
    3) similarity_search(ê¸°ë³¸) ë˜ëŠ” MMR(ì˜µì…˜)ë¡œ ê²€ìƒ‰
    """
    q_norm = normalize_query_colloquial(user_query)     # CANON ì¹˜í™˜
    q_fmt  = format_query_for_e5(q_norm)                # E5 query: ì ‘ë‘ì‚¬
    if USE_MMR:
        fetch_k = max(k * FETCH_MULT, k)
        docs = db.max_marginal_relevance_search(
            q_fmt, k=k, fetch_k=fetch_k, lambda_mult=LAMBDA_MULT
        )
    else:
        docs = db.similarity_search(q_fmt, k=k)
    return docs, q_fmt

def search_meds(med_db, user_query: str, k: int = K_MED):
    """
    ìƒë¹„ì•½ì€ í‚¤ì›Œë“œ/ìœ ì‚¬ë„ ë§¤ì¹­ìœ¼ë¡œ ì¶©ë¶„í•œ ê²½ìš°ê°€ ë§ìŒ â†’ similarity ì‚¬ìš©
    (ë™ì¼í•˜ê²Œ CANON ì¹˜í™˜ + E5 query í¬ë§·)
    """
    q_norm = normalize_query_colloquial(user_query)
    q_fmt  = format_query_for_e5(q_norm)
    docs = med_db.similarity_search(q_fmt, k=k)
    return docs

# -------------------------
# ë©”ì¸ ë£¨í”„(ì½˜ì†” ì¸í„°í˜ì´ìŠ¤)
# -------------------------
if __name__ == "__main__":
    # 1) ì¸ë±ìŠ¤ ì¤€ë¹„(ì¦ìƒ ì „ìš©/ì „ì²´ ì¸ë±ìŠ¤ ëª¨ë‘ êµ¬ì¶• í›„ í”Œë˜ê·¸ë¡œ ì„ íƒ)
    disease_db_sym = build_or_load_faiss_disease_symptom_only()   # ì¦ìƒ ì „ìš© ì¸ë±ìŠ¤
    disease_db_all = build_or_load_faiss_disease()                 # ì „ì²´ ì„¹ì…˜ ì¸ë±ìŠ¤
    disease_db     = disease_db_sym if USE_SYMPTOM_ONLY else disease_db_all

    # âœ… í‰ê°€/ë””ë²„ê¹… í¸ì˜ë¥¼ ìœ„í•´ ëª¨ë“ˆ ì „ì—­ìœ¼ë¡œë„ ë…¸ì¶œ
    globals()["disease_db"] = disease_db

    # 2) ìƒë¹„ì•½/ì˜ë£Œì„± ì¸ë±ìŠ¤ ì¤€ë¹„
    med_db         = build_or_load_faiss_meds()
    medicalness_db = build_or_load_medicalness_index()

    # 3) ì˜ë£Œì„± íŒë³„ ë¦¬íŠ¸ë¦¬ë²„ (k=MED_TOPK)
    medicalness_retriever = medicalness_db.as_retriever(search_kwargs={"k": MED_TOPK})

    # 4) ì½˜ì†” ëŒ€í™” ë£¨í”„ ì‹œì‘
    while True:
        user_input = input("\nğŸ©º ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ").strip()
        if user_input.lower() in ["exit", "ì¢…ë£Œ", "quit"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 4-1) ì˜ë£Œì„± íŒë³„: ì˜ë£Œ ê´€ë ¨ì´ ì•„ë‹ˆë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ì‘ë‹µ
        if is_medical_by_rag(user_input, medicalness_retriever):
            # 4-2) JSON ì‚¬ì „ ê¸°ë°˜ ì •ê·œí™” â†’ CANON ì¹˜í™˜ â†’ E5 query í¬ë§·
            normalized_input = normalize_query_dict(user_input)
            print(f"[ì •ê·œí™”] {user_input}  â†’  {normalized_input}")

            # 4-3) ì§ˆë³‘/ì•½í’ˆ ê²€ìƒ‰
            disease_docs, q_fmt = search_diseases(disease_db, normalized_input, k=K_DISEASE)
            med_docs            = search_meds(med_db, normalized_input, k=K_MED)

            # 4-4) Evidence Gating ì ìˆ˜ ì‚°ì¶œ
            avg_sim, max_sim, n_docs = evidence_score(disease_docs, q_fmt)

            if evidence_ok(avg_sim, max_sim, n_docs):
                # ê·¼ê±° ì¶©ë¶„ â†’ ì¼ë°˜ ëª¨ë“œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±(ë³‘ëª… ê¸°ì¤€ ì¤‘ë³µ ì œê±°)
                disease_docs_dedup = dedupe_by_disease(disease_docs, MAX_DISEASES)
                disease_context = "\n---\n".join([doc.page_content for doc in disease_docs_dedup])
                med_context     = "\n---\n".join([doc.page_content for doc in med_docs[:MAX_MEDS]])
                final_context   = f"[ì§ˆë³‘ ì •ë³´]\n{disease_context}\n\n[ìƒë¹„ì•½ ì •ë³´]\n{med_context}"

                messages = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "system", "content": final_context[:CTX_CHARS]},
                    {"role": "user",   "content": user_input},
                ]

                # (ë””ë²„ê·¸) LLM ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
                print("\nğŸ§© [LLM ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°]")
                print(final_context[:CTX_CHARS])
                print("-" * 60)

            else:
                # ê·¼ê±° ë¶€ì¡± â†’ ì•ˆì „ ëª¨ë“œ: ì§„ë‹¨/ì•½ì¶”ì²œ ëŒ€ì‹  ì¶”ê°€ ì§ˆë¬¸/ì‘ê¸‰ê¸°ì¤€/ìƒí™œê´€ë¦¬
                safe_hint = (
                    "í˜„ì¬ ê²€ìƒ‰ ê·¼ê±°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.\n"
                    f"- avg_sim={avg_sim:.3f}, max_sim={max_sim:.3f}, n_docs={n_docs}\n"
                    "ì§„ë‹¨/ì•½ì¶”ì²œ ëŒ€ì‹ , ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì •ë³´ì™€ ì‘ê¸‰ ê¸°ì¤€, ìƒí™œ ê´€ë¦¬ë§Œ ì•ˆë‚´í•©ë‹ˆë‹¤."
                )
                print("\nğŸ§¯ [Evidence Gating] " + safe_hint)
                messages = [
                    {"role": "system", "content": SAFE_PROMPT},
                    {"role": "user",   "content": user_input},
                ]

        else:
            # ë¹„ì˜ë£Œ ì§ˆë¬¸: ì¼ë°˜ ëŒ€í™” í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"},
                {"role": "user",   "content": user_input}
            ]

        # 4-5) A.X 4.0 í˜¸ì¶œ ë° ì¶œë ¥
        try:
            answer = chat_with_ax(messages)
            print("\nğŸ§¾ [A.X 4.0 ì‘ë‹µ ê²°ê³¼]")
            print(answer)
        except Exception as e:
            print(f"[ì˜¤ë¥˜] A.X 4.0 API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
